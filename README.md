# PPoH: Proximal Policy Optimization with Trust Region on Trajectories

This repository contains an implementation of the PPoH algorithm, a modification of Proximal Policy Optimization (PPO) that enforces a trust region constraint on the trajectories generated by the old and new policies. This enhancement improves convergence and overall performance while preserving the simplicity of the original algorithm through minimal changes.

## Features

* **Trust Region on Trajectories**: Enforces a constraint to keep the KL divergence between old and new policy trajectories within a specified bound.
* **Minimal Modifications**: Builds upon the standard PPO framework with only a few additional lines of code.
* **Flexible Configuration**: Easily switch between the original PPO and the modified algorithm via command-line flags.
* **Example Environments**: Includes support for common Gym environments such as `LunarLander-v3`.

## Prerequisites

* [Docker](https://www.docker.com/) installed on your system.
* [Git](https://git-scm.com/) for cloning the repository.

## Getting Started

Follow these steps to get up and running:

1. **Clone the repository**

   ```bash
   git clone https://github.com/rbouftini/ppoh.git
   cd ppoh
   ```

2. **Build the Docker image**

   ```bash
   docker build -t ppoh .
   ```

3. **Start a Docker container with a Bash session**

   * **PowerShell**

     ```powershell
     docker run -it --mount "type=bind,src=$($pwd),target=/ppoh" ppoh bash
     ```

   * **bash/zsh**

     ```bash
     docker run -it --mount type=bind,src=$(pwd),target=/ppoh ppoh bash
     ```

4. **Run experiments**

   Inside the container, execute the `run.py` script with your desired configuration. For example:

   ```bash
   python run.py --alg=new --env=LunarLander-v3 --num-eps=100
   ```

   * `--alg` : Choose between `old` (standard PPO) and `new` (PPoH).
   * `--env` : Gym environment name (e.g., `LunarLander-v3`).
   * `--num-eps` : Number of episodes to train.

## Repository Structure

```
ppoh/
├── Dockerfile         # Docker image build instructions
├── run.py             # Main script to start training
├── ppo/               # PPO implementation and modifications
│   ├── agent.py       # Policy and value network definitions
│   ├── ppo.py         # Standard PPO routines
│   └── ppoh.py        # Trust-region trajectory modifications
├── configs/           # Configuration files for experiments
├── logs/              # Training logs and TensorBoard files
└── README.md          # Project overview and instructions
```

## Contributing

Contributions are welcome! Please open issues or submit pull requests for bug fixes, new features, or improvements.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
